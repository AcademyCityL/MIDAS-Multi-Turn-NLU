# default parameters
seed = 10
name = 'roberta_large_intent_detection_ft_m2m'
retain_ckp = true
# [PARAMSGRID]
# MODEL_-attn_mode=[
# ]
[EXPERIMENT]
accelerator = 'gpu'
batch_size = 32
epochs = 3
save_path = 'results/experiments/ft/'
optimizer = 'adamw'
optimizer_params = {}
lrscheduler = 'constantwarmup'
lrscheduler_params={}
lr = 0.000005
warmup = 0.1
# steps = 3000
stop_strategy = 'epoch_stop'
stop_patience = 10
monitor = 'val_loss'
mode = 'min'
temperature = 20
str_name = 'id_m2m_no_adjust_labels'

[TEACHERS]
teacher_list = ['t1']
t1 = {type='RoBERTa', name='roberta-large',head={type='id',nclasses=15},freeze=false}

[DATA]
names = ['M2MDCIDSFPOSDataset']
num_workers = 4
max_seq_len = 512







